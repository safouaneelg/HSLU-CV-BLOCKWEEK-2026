{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22c3b9d",
   "metadata": {},
   "source": [
    "## 1. What is PyTorch? The Core Idea ðŸ’¡\n",
    "\n",
    "At its heart, PyTorch is a Python library for scientific computing that's loved by researchers and developers for its simplicity and power. It provides two main features that make it perfect for deep learning:\n",
    "\n",
    "* **Tensors:** These are multi-dimensional arrays, similar to NumPy arrays. The superpower of PyTorch tensors is that they can be easily moved to a **GPU** for massive speedups in computation.\n",
    "* **Automatic Differentiation:** PyTorch can automatically calculate gradients (derivatives). This is the magic that allows neural networks to \"learn\" from data, and it's managed by a system called `autograd`.\n",
    "\n",
    "It's known for being \"Pythonic,\" meaning its design feels natural and intuitive to anyone familiar with Python. Let's dive into the most fundamental building block: the Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68e4017b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Print the PyTorch version we are using\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59adf4fe",
   "metadata": {},
   "source": [
    "## 2. The Building Blocks: PyTorch Tensors ðŸ§±\n",
    "\n",
    "Everything in PyTorch revolves around the **Tensor**. A tensor is a number, vector, matrix, or any n-dimensional array. It's the primary data structure we'll be working with.\n",
    "\n",
    "### Creating Tensors\n",
    "\n",
    "You can create tensors in several ways.\n",
    "\n",
    "#### From a Python list:\n",
    "\n",
    "The most basic way is to create a tensor directly from a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a6868a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a simple 1-dimensional tensor (a vector)\n",
    "data = [[1, 2], [3, 4]]\n",
    "my_tensor = torch.tensor(data)\n",
    "\n",
    "my_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8a3b9",
   "metadata": {},
   "source": [
    "#### From a NumPy array:\n",
    "\n",
    "PyTorch integrates seamlessly with NumPy. You can create a tensor from a NumPy array and vice-versa. This is incredibly useful since many data processing libraries (like Scikit-learn, Pandas) are built on NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d02a9fa-6820-4a61-a21d-52a6182e0963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy array:\n",
      " [[5. 6.]\n",
      " [7. 8.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a NumPy array\n",
    "numpy_array = np.array([[5., 6.], [7., 8.]])\n",
    "print(f\"NumPy array:\\n {numpy_array}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cec8c942-ffbf-4f6d-97bc-d051f6a82878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor from NumPy:\n",
      " tensor([[5., 6.],\n",
      "        [7., 8.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Convert NumPy array to a PyTorch tensor\n",
    "numpy_to_tensor = torch.from_numpy(numpy_array)\n",
    "print(f\"Tensor from NumPy:\\n {numpy_to_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdbe1d0",
   "metadata": {},
   "source": [
    "#### Using built-in functions:\n",
    "\n",
    "PyTorch also provides functions to create tensors with specific shapes and values, which is very common when initializing a neural network's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ef3b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of ones:\n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "\n",
      "Tensor of zeros:\n",
      " tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "\n",
      "Random tensor:\n",
      " tensor([[-1.9324e+00, -2.2962e+00,  2.3067e-01, -5.9618e-01],\n",
      "        [-1.8841e+00, -1.3308e+00,  1.0977e+00, -6.1980e-01],\n",
      "        [ 3.5177e-01, -1.3615e+00,  4.1155e-05,  1.0963e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor of shape (3, 4) with all ones\n",
    "ones_tensor = torch.ones(3, 4)\n",
    "print(f\"Tensor of ones:\\n {ones_tensor}\\n\")\n",
    "\n",
    "# Create a tensor of shape (3, 4) with all zeros\n",
    "zeros_tensor = torch.zeros(3, 4)\n",
    "print(f\"Tensor of zeros:\\n {zeros_tensor}\\n\")\n",
    "\n",
    "# Create a tensor of shape (3, 4) with random numbers from a standard normal distribution\n",
    "random_tensor = torch.randn(3, 4)\n",
    "print(f\"Random tensor:\\n {random_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e91ccd1",
   "metadata": {},
   "source": [
    "### Tensor Attributes\n",
    "\n",
    "A tensor has attributes that describe its `shape`, `dtype` (data type), and the `device` (CPU or GPU) where it's stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dbcf09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect our random tensor\n",
    "print(f\"Shape of tensor: {random_tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {random_tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {random_tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8731c119",
   "metadata": {},
   "source": [
    "### Moving Tensors to the GPU\n",
    "\n",
    "One of the key advantages of PyTorch is its ability to perform computations on a GPU for significant speed-ups. You can check if a GPU is available and move your tensor to it using the `.to()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f25351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Silicon GPU (MPS) is available! We'll use the GPU.\n",
      "\n",
      "Our random tensor is now on: mps:0\n"
     ]
    }
   ],
   "source": [
    "# 1. Check for Apple Silicon GPU (MPS)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"Apple Silicon GPU (MPS) is available! We'll use the GPU.\")\n",
    "# 2. Check for NVIDIA GPU (CUDA)\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"NVIDIA GPU (CUDA) is available! We'll use the GPU.\")\n",
    "# 3. Fallback to CPU\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"No GPU available, we'll use the CPU.\")\n",
    "\n",
    "# --- Using the Device ---\n",
    "\n",
    "# Move our tensor to the selected device\n",
    "tensor_on_device = random_tensor.to(device)\n",
    "\n",
    "random_tensor = torch.randn(3, 4)\n",
    "print(f\"\\nOur random tensor is now on: {tensor_on_device.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6f5ac",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "\n",
    "Operations on tensors work much like you'd expect. We can perform standard arithmetic in an intuitive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3862977a-29f7-462e-9093-4cf958301091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 : tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "t2 : tensor([[10, 10],\n",
      "        [10, 10]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Let's create two tensors with the SAME dtype\n",
    "t1 = torch.tensor([[1, 2], [3, 4]], dtype=torch.int32)\n",
    "t2 = torch.ones(2, 2, dtype=torch.int32) * 10 # Creates a 2x2 tensor of 10s\n",
    "\n",
    "print(f\"t1 : {t1}\")\n",
    "print(f\"t2 : {t2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f21130a-e38a-4a21-8b1c-2b73f7b27f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 dtype: torch.int32\n",
      "t2 dtype: torch.int32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check their dtypes\n",
    "print(f\"t1 dtype: {t1.dtype}\")\n",
    "print(f\"t2 dtype: {t2.dtype}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf7c4fcb-8ccc-4f35-ba93-b9b3e7ecfd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition:\n",
      " tensor([[11, 12],\n",
      "        [13, 14]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Addition\n",
    "print(\"Addition:\\n\", t1 + t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22dcd643-b5a3-45af-832f-249c6332d2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multiplication:\n",
      " tensor([[10, 20],\n",
      "        [30, 40]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Element-wise multiplication\n",
    "print(\"\\nMultiplication:\\n\", t1 * t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7746ed1-1c84-4f14-96fa-73bfa8cb5b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrix Multiplication:\n",
      " tensor([[30, 30],\n",
      "        [70, 70]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "print(\"\\nMatrix Multiplication:\\n\", t1.matmul(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6af186-6dd1-403d-8f40-1cc0c022c773",
   "metadata": {},
   "source": [
    "## What are Directed Acyclic Graphs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300fc114-93ed-4e08-a98f-335876c990e5",
   "metadata": {},
   "source": [
    "* **Graph:** It's a structure made of **nodes** (the tasks or data) and **edges** (the dependencies or operations connecting them).\n",
    "* **Directed:** The edges have a direction, represented by arrows. This means the relationship is one-way. If you need to mix flour and eggs (Inputs) to make batter (Output), the arrow points from the ingredients *to* the batter, not the other way around. The flow is fixed.\n",
    "* **Acyclic:** This is the crucial part. It means there are **no cycles or loops**. You can never start at a node, follow the directed arrows, and end up back at the same node. In our recipe analogy, you can't un-bake a cake to get the batter back. The process only moves forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4fd89b-5806-496f-9e8a-305a3e4e1056",
   "metadata": {},
   "source": [
    "* In short, a DAG is a flowchart with no loops, where every step flows in one direction from start to finish.\n",
    "* In PyTorch, the DAG is called a computational graph. It's built automatically in the background to keep track of every operation you perform on your tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "063e8cbb-064a-4912-ac9c-8df82a684dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires_grad=True tells PyTorch to track operations for this tensor\n",
    "a = torch.tensor([2.0], requires_grad=True)\n",
    "b = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "c = a * b\n",
    "d = c + 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c8b165-05a2-4c94-8864-b93da2028926",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3738323657.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m(Tensor a) -----> (*) -----> (Tensor c) -----> (+) -----> (Tensor d)\u001b[39m\n     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "  (Tensor a) -----> (*) -----> (Tensor c) -----> (+) -----> (Tensor d)\n",
    "                       ^                            ^\n",
    "                       |                            |\n",
    "  (Tensor b) ----------+         (Scalar 5.0) ------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507a614f-5a1f-4181-853e-4d2014eb4968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
